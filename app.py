import streamlit as st
import os
import chromadb
from google import genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings as GGClientEmbedings
from chromadb.api.models.Collection import Collection

if "GEMINI_API_KEY" not in st.secrets:
    st.error("HATA: UygulamayÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in GEMINI_API_KEY gizli anahtarÄ± bulunamadÄ±. LÃ¼tfen Streamlit Cloud ayarlarÄ±na ekleyin.")
    st.stop()
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# --- Sabitler ve Ayarlar ---
CHROMA_DB_PATH = "chroma_db"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 
LLM_MODEL = "gemini-2.5-flash"

# --- RAG Prompt ---
RAG_PROMPT_TEMPLATE = """
Sen, Python programlama dili hakkÄ±nda uzman bir eÄŸitmen ve yardÄ±mcÄ±sÄ±n. 
KullanÄ±cÄ±nÄ±n sorularÄ±nÄ± yalnÄ±zca aÅŸaÄŸÄ±daki BAÄLAM'da bulunan dokÃ¼manlardan aldÄ±ÄŸÄ±n bilgilerle yanÄ±tla.
EÄŸer baÄŸlamda soruya cevap bulamÄ±yorsan, "Bu soruya mevcut Python dokÃ¼manlarÄ±mda yanÄ±t bulamÄ±yorum." de.

BAÄLAM: {context}

SORU: {question}
"""

# --- BaÅŸlangÄ±Ã§ Fonksiyonu (VektÃ¶r DB'yi GÃ¼venli YÃ¼kleme) ---
@st.cache_resource 
def setup_rag_pipeline() -> Collection | None:
    # API Key kontrolÃ¼nÃ¼ yukarÄ±ya taÅŸÄ±dÄ±k. Burada tekrar kontrol etmeye gerek yok.

    try:
        # 1. VektÃ¶rleÅŸtirme Modeli
        embedding_function = GGClientEmbedings(
            model="text-embedding-004"
        )
        
        # 2. VektÃ¶r VeritabanÄ±nÄ± YÃ¼kleme (Koleksiyonun Var Olup OlmadÄ±ÄŸÄ±nÄ± Kontrol Etmek Gerekir)
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

        # Koleksiyonun varlÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in list_collections kullanÄ±lÄ±r.
        existing_collections = client.list_collections()
        if "python_docs" not in [c.name for c in existing_collections]:
            # KRÄ°TÄ°K DÃœZELTME: Koleksiyon yoksa hata fÄ±rlatmak yerine uyarÄ± veriyoruz.
            # Bu, uygulamanÄ±n Ã§Ã¶kmesini engeller.
            st.warning("""
            ğŸš¨ UYARI: 'python_docs' RAG koleksiyonu bulunamadÄ±. 
            LÃ¼tfen verileri yÃ¼kleme (embed etme) betiÄŸini (Ã¶rn. 'rag_pipeline.py' veya 'setup.py') Streamlit ortamÄ±nda ayrÄ±ca Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun. 
            Bu uygulama RAG olmadan Ã§alÄ±ÅŸamayacaktÄ±r.
            """)
            return None
        
        # Koleksiyon var, gÃ¼venle alabiliriz
        collection = client.get_collection(
            name="python_docs",
            embedding_function=embedding_function
        )

        # Gemini LLM'i configure et
        genai.configure(api_key=GEMINI_API_KEY)
        return collection
        
    except Exception as e:
        # DiÄŸer olasÄ± hatalarÄ± (dosya eriÅŸimi vb.) yakalarÄ±z
        st.error(f"RAG bileÅŸenleri yÃ¼klenirken beklenmedik bir hata oluÅŸtu: {e}")
        return None

# --- Chat Fonksiyonu ---
def generate_response(collection, prompt: str):
    # 1. Geri Ã‡ekme (Retrieval)
    results = collection.query(
        query_texts=[prompt],
        n_results=3
    )
    
    # 2. BaÄŸlamÄ± (Context) BirleÅŸtirme
    context = "\n---\n".join([doc for docs in results.get('documents', [[]]) for doc in docs])
    
    # 3. Prompt'u HazÄ±rlama
    final_prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=prompt)
    
    # 4. Ãœretme (Generation)
    response = genai.GenerativeModel(model_name=LLM_MODEL).generate_content(
        contents=final_prompt,
        config=genai.types.GenerateContentConfig(
            temperature=0.1
        )
    )
    return response.text


# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Akbank Python RAG Chatbot")
st.title("ğŸ Python Ã–ÄŸrenme RAG Chatbotu (Temel)")

# GÃ¼venli yÃ¼kleme (Hata durumunda None dÃ¶ner)
collection = setup_rag_pipeline()

if collection:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Python ile ilgili sorunuz nedir?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("YanÄ±t aranÄ±yor..."):
                try:
                    response = generate_response(collection, prompt) 
                    st.markdown(response)
                except Exception as e:
                    response = f"ÃœzgÃ¼nÃ¼m, yanÄ±t oluÅŸturulurken bir hata oluÅŸtu: {e}"
                    st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})
else:
    # Koleksiyon bulunamadÄ±ÄŸÄ±nda chat arayÃ¼zÃ¼nÃ¼ gÃ¶sterme
    st.info("LÃ¼tfen RAG veritabanÄ±nÄ± yÃ¼kledikten sonra sayfayÄ± yenileyin.")