import streamlit as st
import os
import chromadb
from google import genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings as GGClientEmbedings
from chromadb.api.models.Collection import Collection


# --- Sabitler ve Ayarlar ---
CHROMA_DB_PATH = "chroma_db"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 
LLM_MODEL = "gemini-2.5-flash"

# --- RAG Prompt ---
RAG_PROMPT_TEMPLATE = """
Sen, Python programlama dili hakkÄ±nda uzman bir eÄŸitmen ve yardÄ±mcÄ±sÄ±n. 
KullanÄ±cÄ±nÄ±n sorularÄ±nÄ± yalnÄ±zca aÅŸaÄŸÄ±daki BAÄLAM'da bulunan dokÃ¼manlardan aldÄ±ÄŸÄ±n bilgilerle yanÄ±tla.
EÄŸer baÄŸlamda soruya cevap bulamÄ±yorsan, "Bu soruya mevcut Python dokÃ¼manlarÄ±mda yanÄ±t bulamÄ±yorum." de.

BAÄLAM: {context}

SORU: {question}
"""

# --- BaÅŸlangÄ±Ã§ Fonksiyonu (VektÃ¶r DB'yi YÃ¼kleme) ---
@st.cache_resource 
def setup_rag_pipeline() -> Collection:
    if not GEMINI_API_KEY:
        st.error("HATA: GEMINI_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±.")
        return None

    try:
        # 1. VektÃ¶rleÅŸtirme Modeli (Google'Ä±n kendi kÃ¼tÃ¼phanesi)
        embedding_function = GGClientEmbedings(
             
            model="text-embedding-004"
        )
        
        # 2. VektÃ¶r VeritabanÄ±nÄ± YÃ¼kleme
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        collection = client.get_collection(
            name="python_docs",
            embedding_function=embedding_function
        )
        
        # EÄŸer collection boÅŸsa (veri yÃ¼klenmediyse), hata ver
        if collection.count() == 0:
            st.warning("VeritabanÄ± boÅŸ gÃ¶rÃ¼nÃ¼yor. LÃ¼tfen 'rag_pipeline.py' dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun.")

        # Gemini LLM'i configure et
        genai.configure(api_key=GEMINI_API_KEY)
        return collection
        
    except Exception as e:
        st.error(f"RAG bileÅŸenleri yÃ¼klenirken KRÄ°TÄ°K HATA oluÅŸtu: {e}")
        return None

# --- Chat Fonksiyonu ---
def generate_response(collection, prompt: str):
    # 1. Geri Ã‡ekme (Retrieval)
    results = collection.query(
        query_texts=[prompt],
        n_results=3
    )
    
    # 2. BaÄŸlamÄ± (Context) BirleÅŸtirme
    context = "\n---\n".join([doc for docs in results['documents'] for doc in docs])
    
    # 3. Prompt'u HazÄ±rlama
    final_prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=prompt)
    
    # 4. Ãœretme (Generation)
    response = genai.GenerativeModel(model_name=LLM_MODEL).generate_content(
        contents=final_prompt,
        config=genai.types.GenerateContentConfig(
            temperature=0.1
        )
    )
    return response.text


# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Akbank Python RAG Chatbot")
st.title("ğŸ Python Ã–ÄŸrenme RAG Chatbotu (Temel)")
collection = setup_rag_pipeline()

if collection:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Python ile ilgili sorunuz nedir?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("YanÄ±t aranÄ±yor..."):
                try:
                    response = generate_response(collection, prompt) 
                    st.markdown(response)
                except Exception as e:
                    response = f"ÃœzgÃ¼nÃ¼m, yanÄ±t oluÅŸturulurken bir hata oluÅŸtu: {e}"
                    st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})